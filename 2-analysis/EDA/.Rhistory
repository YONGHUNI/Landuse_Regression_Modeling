initial = xgb_res_init,
iter = 6#,
#control = control_grid(save_pred = TRUE)
)
xgb_bayes_res_init <- tune_bayes(
xgb_wf_init,
pm_folds_init,
#initial = xgb_res_init,
iter = 6#,
#control = control_grid(save_pred = TRUE)
)
xgb_bayes_res_init <- tune_bayes(
#xgb_wf_init,
pm_folds_init,
initial = xgb_res_init,
iter = 6#,
#control = control_grid(save_pred = TRUE)
)
xgb_bayes_res_init <- tune_bayes(
xgb_init,
pm_folds_init,
#initial = xgb_res_init,
iter = 6#,
#control = control_grid(save_pred = TRUE)
)
xgb_bayes_res_init <- tune_bayes(
xgb_init,
pm_folds_init,
pm_recipe_init,
#initial = xgb_res_init,
iter = 6#,
#control = control_grid(save_pred = TRUE)
)
xgb_bayes_res_init <- tune_bayes(
xgb_init,
pm_recipe_init,
pm_folds_init,
#initial = xgb_res_init,
iter = 6#,
#control = control_grid(save_pred = TRUE)
)
xgb_res_init <- tune_grid(
xgb_wf_init,
resamples = pm_folds_init,
#grid = xgb_grid_init,
control = control_grid(save_pred = T,
verbose = T)
)
finalize(mtry(), baked_trained_pm_init)
set.seed(seed)
boost_tree() %>%
set_args(
#mtry = tune(), # colsample_bytree
trees = tune(),         # nrounds
min_n = tune(),         # min_child_weight
tree_depth = tune(),    # max_depth
learn_rate = tune(),    # eta
loss_reduction = tune(),# gamma
sample_size = tune(),   # subsample
stop_iter = tune()      # early stop
) %>%
set_engine("xgboost") %>%
set_mode("regression") -> xgb_init
# # of simulations
grid_nsim <- 200
grid_random(
trees(),
tree_depth(),
min_n(),
loss_reduction(),
sample_size = sample_prop(),
finalize(mtry(), baked_trained_pm_init),
learn_rate(),
stop_iter(),
size = grid_nsim) -> xgb_grid_init
xgb_grid_init
xgb_wf_init <- workflow() %>%
add_recipe(pm_recipe_init) %>%
add_model(xgb_init)
xgb_wf_init
xgb_res_init <- tune_grid(
xgb_wf_init,
resamples = pm_folds_init,
#grid = xgb_grid_init,
control = control_grid(save_pred = T,
verbose = T)
)
xgb_bayes_res_init <- tune_bayes(
xgb_init,
pm_recipe_init,
pm_folds_init,
#initial = xgb_res_init,
iter = 6#,
#control = control_grid(save_pred = TRUE)
)
stopcl(cons)
set.seed(seed)
cons <- register_cores()
xgb_res_init <- tune_grid(
xgb_wf_init,
resamples = pm_folds_init,
grid = xgb_grid_init,
control = control_grid(save_pred = T,
verbose = T)
)
set.seed(seed)
boost_tree() %>%
set_args(
mtry = finalize(mtry(), baked_trained_pm_init), # colsample_bytree
trees = tune(),         # nrounds
min_n = tune(),         # min_child_weight
tree_depth = tune(),    # max_depth
learn_rate = tune(),    # eta
loss_reduction = tune(),# gamma
sample_size = tune(),   # subsample
stop_iter = tune()      # early stop
) %>%
set_engine("xgboost") %>%
set_mode("regression") -> xgb_init
# # of simulations
grid_nsim <- 200
grid_random(
trees(),
tree_depth(),
min_n(),
loss_reduction(),
sample_size = sample_prop(),
#finalize(mtry(), baked_trained_pm_init),
learn_rate(),
stop_iter(),
size = grid_nsim) -> xgb_grid_init
xgb_grid_init
xgb_wf_init <- workflow() %>%
add_recipe(pm_recipe_init) %>%
add_model(xgb_init)
xgb_wf_init
set.seed(seed)
cons <- register_cores()
xgb_res_init <- tune_grid(
xgb_wf_init,
resamples = pm_folds_init,
grid = xgb_grid_init,
control = control_grid(save_pred = T,
verbose = T)
)
stopcl(cons)
xgb_res_init
stopcl(cons)
set.seed(seed)
boost_tree() %>%
set_args(
#mtry = tune(), # colsample_bytree
trees = tune(),         # nrounds
min_n = tune(),         # min_child_weight
tree_depth = tune(),    # max_depth
learn_rate = tune(),    # eta
loss_reduction = tune(),# gamma
sample_size = tune(),   # subsample
stop_iter = tune()      # early stop
) %>%
set_engine("xgboost") %>%
set_mode("regression") -> xgb_init
# # of simulations
grid_nsim <- 200
grid_random(
trees(),
tree_depth(),
min_n(),
loss_reduction(),
sample_size = sample_prop(),
#finalize(mtry(), baked_trained_pm_init),
learn_rate(),
stop_iter(),
size = grid_nsim) -> xgb_grid_init
xgb_grid_init
xgb_wf_init <- workflow() %>%
add_recipe(pm_recipe_init) %>%
add_model(xgb_init)
xgb_wf_init
set.seed(seed)
cons <- register_cores()
xgb_res_init <- tune_grid(
xgb_wf_init,
resamples = pm_folds_init,
grid = xgb_grid_init,
control = control_grid(save_pred = T,
verbose = T)
)
stopcl(cons)
xgb_res_init
cons <- register_cores()
stopcl(cons)
set.seed(seed)
cons <- register_cores()
xgb_bayes_res_init <- tune_bayes(
xgb_wf_init,
pm_folds_init,
initial = xgb_res_init,
iter = 6#,
#control = control_grid(save_pred = TRUE)
)
stopcl(cons)
set.seed(seed)
cons <- register_cores()
xgb_bayes_res_init <- tune_bayes(
xgb_wf_init,
pm_folds_init,
initial = xgb_res_init,
iter = 6,
control = control_grid(save_pred = TRUE,allow_par = TRUE)
)
set.seed(seed)
cons <- register_cores()
xgb_bayes_res_init <- tune_bayes(
xgb_wf_init,
pm_folds_init,
initial = xgb_res_init,
iter = 6#,
#control = control_grid(save_pred = TRUE,allow_par = TRUE)
)
stopcl(cons)
set.seed(seed)
cons <- register_cores()
xgb_bayes_res_init <- tune_bayes(
xgb_wf_init,
pm_folds_init,
initial = xgb_res_init,
iter = 6,
control = control_bayes(save_pred = TRUE,allow_par = TRUE)
)
stopcl(cons)
set.seed(seed)
cons <- register_cores()
xgb_bayes_res_init <- tune_bayes(
xgb_wf_init,
pm_folds_init,
initial = xgb_res_init,
iter = 6,
parallel_over = "everything",
control = control_bayes(save_pred = TRUE,allow_par = TRUE)
)
stopcl(cons)
set.seed(seed)
cons <- register_cores()
xgb_bayes_res_init <- tune_bayes(
xgb_wf_init,
pm_folds_init,
initial = xgb_res_init,
iter = 6,
control = control_bayes(save_pred = TRUE,
allow_par = TRUE,
parallel_over = "everything")
)
stopcl(cons)
xgb_bayes_res_init
xgb_bayes_res_init
xgb_bayes_res_init$.metrics
show_best(xgb_bayes_res_init)
xgb_bayes_res_init
show_best(xgb_bayes_res_init)
show_best(xgb_bayes_res_init,"mape")
show_best(xgb_bayes_res_init,"MAPE")
show_best(xgb_bayes_res_init,metric = "MAPE")
show_best(xgb_bayes_res_init,metric = "mape")
show_best(xgb_bayes_res_init,metric = mape())
show_best(xgb_bayes_res_init,metric = mape())
show_best(xgb_bayes_res_init,metric = mape
)
show_best(xgb_bayes_res_init,metric = mape)
show_best(xgb_bayes_res_init,metric = "mape")
show_best(xgb_bayes_res_init,metric = yardstick::mape())
show_best(xgb_bayes_res_init,metric = "mape")
show_best(xgb_bayes_res_init,metric = "rmse")
best_xgb_init <- select_best(xgb_res_init, "rmse")
best_xgb_init <- select_best(xgb_res_init,metric =  "rmse")
xgb_wf_init
set.seed(seed)
final_wf_xgb_init <- finalize_workflow(
xgb_wf_init,
best_xgb_init
)
final_wf_xgb_init
metrics <- metric_set(yardstick::mae,
yardstick::mse,
yardstick::rmse,
yardstick::mpe,
yardstick::mape,
yardstick::rsq
)
yardstick::mpe
yardstick::mpe()
metrics <- metric_set(yardstick::mae,
yardstick::mse,
yardstick::rmse,
yardstick::mpe,
yardstick::mape,
yardstick::rsq
)
metrics <- metric_set(yardstick::mae,
yardstick::rmse,
yardstick::mpe,
yardstick::mape,
yardstick::rsq
)
yardstick::mpvar_splt_inite()
var_splt_init
set.seed(seed)
final_res_xgb_init <- last_fit(final_wf_xgb_init,
split = var_splt_init,
metrics = metrics
)
collect_metrics(final_res_xgb_init)
summary(var_splt_init$data$mobile_PM2.5)
skimr::skim(var_splt_init$data$mobile_PM2.5)
traindata_init <- bake(pm_recipe_init, new_data = NULL)
traindata_init <- bake(pm_recipe_init %>% prep, new_data = NULL)
traindata_init <- bake(prep(pm_recipe_init), new_data = NULL)
traindata_init
final_res_xgb_init
xgb_init_unif <- extract_fit_engine(final_res_xgb_init) %>%
xgboost.unify(data = traindata_init)
traindata_init <- bake(prep(pm_recipe_init), new_data = NULL)
xgb_init_unif <- extract_fit_engine(final_res_xgb_init) %>%
xgboost.unify(data = traindata_init)
library(tidymodels)
library(treeshap)
library(fastshap)
# data handling
library(tidyverse)
library(data.table)
library(lubridate)
library(reshape2)
library(doParallel)
# spatial data(vector)
# library(sf)
# spatial data(raster)
# library(terra)
# library(tidyterra)
# R-Python framework
# library(reticulate)
# visualization
library(magick)
library(gganimate)
library(viridis)
library(corrplot)
# spatial data visualization
# library(tmap)
# tmap_options(check.and.fix = TRUE)
# library(leafem)
library(shapviz)
traindata_init <- bake(prep(pm_recipe_init), new_data = NULL)
xgb_init_unif <- extract_fit_engine(final_res_xgb_init) %>%
xgboost.unify(data = traindata_init)
traindata_init <- bake(prep(pm_recipe_init), new_data = NULL)
xgb_init_unif <- extract_fit_engine(final_res_xgb_init) %>%
xgboost.unify(data = traindata_init)
tshap_xgb_init <- treeshap(xgb_init_unif, traindata_init,interactions = T)
plot_feature_importance(tshap_xgb_init)
(sv_importance(sv_xgb_init, kind = "both",
show_numbers = TRUE,
bar_width = 1,
bee_width = 0.2,
fill = "blue",
max_display = 30
)#+
#labs(x = "SHAP 값(XGB)", col =  "특성 값")
) %>%
print -> summary_xgb_init
set.seed(seed)
sv_xgb_init <- shapviz(tshap_xgb_init, X = traindata_init)
(sv_importance(sv_xgb_init, kind = "both",
show_numbers = TRUE,
bar_width = 1,
bee_width = 0.2,
fill = "blue",
max_display = 30
)#+
#labs(x = "SHAP 값(XGB)", col =  "특성 값")
) %>%
print -> summary_xgb_init
tshap_xgb_init
tshap_xgb_init %>% view
tshap_xgb_init$shaps %>% view
(sv_importance(sv_xgb_init, kind = "both",
show_numbers = TRUE,
bar_width = 1,
bee_width = 0.2,
fill = "blue",
max_display = 20
)#+
#labs(x = "SHAP 값(XGB)", col =  "특성 값")
) %>%
print -> summary_xgb_init
treeshap::plot_contribution(tshap_xgb_init)
treeshap::plot_feature_dependence(tshap_xgb_init)
treeshap::plot_feature_dependence(tshap_xgb_init,"dist_airport")
names(traindata_init)
traindata_init[1:33]
names(traindata_init)[1:33]
treeshap::plot_feature_dependence(tshap_xgb_init,names(traindata_init)[1:33])
for (i in 1:33) {
treeshap::plot_feature_dependence(tshap_xgb_init,names(traindata_init)[i])
}
for (i in 1:33) {
treeshap::plot_feature_dependence(tshap_xgb_init,names(traindata_init)[i])
}
treeshap::plot_feature_dependence(tshap_xgb_init,names(traindata_init)[i]) %>% print
treeshap::plot_feature_dependence(tshap_xgb_init,names(traindata_init)[i])
for (i in 1:33) {
treeshap::plot_feature_dependence(tshap_xgb_init,names(traindata_init)[i]) %>% plot()
}
length(sv_xgb_init$X)
nm <- names(sv_xgb_init$X)
sv_dependence(sv_xgb_init,  nm[i], color_var = nm[i])
sv_dependence(sv_xgb_init,  nm[i], color_var = nm[i]) +
geom_smooth(aes(color="non-linear trend", fill="95% conf. interval"),
method = 'loess',formula = 'y ~ x')+
labs(y = "SHAP value", x =  nm[i]) +
scale_x_continuous(labels = my_axis_format)+
geom_hline(yintercept=0, linetype="dashed",
color = "red", linewidth=1) +
scale_fill_manual(NULL, values = 'gray') +
scale_color_manual(NULL, values = 'blue') +
guides(
color=guide_legend(override.aes = list(fill=NA), order=1),
fill=guide_legend(override.aes = list(color=NA), order=2))+
theme(text = element_text(size=20))-> temp
my_axis_format <- function(x) format(x, big.mark = ",", scientific = FALSE)
sv_dependence(sv_xgb_init,  nm[i], color_var = nm[i]) +
geom_smooth(aes(color="non-linear trend", fill="95% conf. interval"),
method = 'loess',formula = 'y ~ x')+
labs(y = "SHAP value", x =  nm[i]) +
scale_x_continuous(labels = my_axis_format)+
geom_hline(yintercept=0, linetype="dashed",
color = "red", linewidth=1) +
scale_fill_manual(NULL, values = 'gray') +
scale_color_manual(NULL, values = 'blue') +
guides(
color=guide_legend(override.aes = list(fill=NA), order=1),
fill=guide_legend(override.aes = list(color=NA), order=2))+
theme(text = element_text(size=20))-> temp
sv_dependence(sv_xgb_init,  nm[i], color_var = nm[i]) +
geom_smooth(aes(color="non-linear trend", fill="95% conf. interval"),
method = 'loess',formula = 'y ~ x')+
labs(y = "SHAP value", x =  nm[i]) +
scale_x_continuous(labels = my_axis_format)+
geom_hline(yintercept=0, linetype="dashed",
color = "red", linewidth=1) +
scale_fill_manual(NULL, values = 'gray') +
scale_color_manual(NULL, values = 'blue') +
guides(
color=guide_legend(override.aes = list(fill=NA), order=1),
fill=guide_legend(override.aes = list(color=NA), order=2))+
theme(text = element_text(size=20))-> temp
#ggsave(filename = paste0("./images/PDP/XGB/maineff/XGB_",
#                         nm[i],"_dep.png"),
#     scale = 1,
#     dpi = 256)
print(temp)
my_axis_format <- function(x) format(x, big.mark = ",", scientific = FALSE)
nm <- names(sv_xgb_init$X)
for (i in 1:length(nm)) {
sv_dependence(sv_xgb_init,  nm[i], color_var = nm[i]) +
geom_smooth(aes(color="non-linear trend", fill="95% conf. interval"),
method = 'loess',formula = 'y ~ x')+
labs(y = "SHAP value", x =  nm[i]) +
scale_x_continuous(labels = my_axis_format)+
geom_hline(yintercept=0, linetype="dashed",
color = "red", linewidth=1) +
scale_fill_manual(NULL, values = 'gray') +
scale_color_manual(NULL, values = 'blue') +
guides(
color=guide_legend(override.aes = list(fill=NA), order=1),
fill=guide_legend(override.aes = list(color=NA), order=2))+
theme(text = element_text(size=20))-> temp
#ggsave(filename = paste0("./images/PDP/XGB/maineff/XGB_",
#                         nm[i],"_dep.png"),
#     scale = 1,
#     dpi = 256)
print(temp)
}
nm
autoplot(xgb_res_init)
show_best(xgb_res_init, metric = "rmse")
best_xgb_init <- select_best(xgb_bayes_res_init,metric =  "rmse")
best_xgb_init <- select_best(c(xgb_bayes_res_init,xgb_bayes_res_init),metric =  "rmse")
xgb_bayes_res_init
best_xgb_init <- select_best(rbind(xgb_bayes_res_init,xgb_bayes_res_init),metric =  "rmse")
best_xgb_init
set.seed(seed)
final_wf_xgb_init <- finalize_workflow(
xgb_wf_init,
best_xgb_init
)
final_wf_xgb_init
metrics <- metric_set(yardstick::mae,
yardstick::rmse,
yardstick::mpe,
yardstick::mape,
yardstick::rsq
)
set.seed(seed)
final_res_xgb_init <- last_fit(final_wf_xgb_init,
split = var_splt_init,
metrics = metrics
)
collect_metrics(final_res_xgb_init)
collect_metrics(final_res_xgb_init)
collect_metrics(final_res_xgb_init)
collect_metrics(final_res_xgb_init)
xgb_bayes_res_init
autoplot(xgb_bayes_res_init)
